{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55365c40-2b04-43a3-8e51-a4f070fc46cc",
   "metadata": {},
   "source": [
    "# <center>Hyperparameter Optimization or Tuning</center>\n",
    "\n",
    "> * is the process of finding best Hyperparameters for a given dataset. \n",
    "> * Best Hyperparameters are those that minimizes the generalization error (not necessarily loss error).\n",
    "> * The critical step is to choose how many different Hyperparameter combinations we are going to test because -  \n",
    "> * greater the no. of Hypermater combinations greater the chance of getting better model and so does greater the computational cost\n",
    ">> **No. of Hyperparameter Combinations ⋉ Better Model ⋉ Computing Cost**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf68e5-d2da-4dec-a6f9-901fe5f8af22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Aim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67de6f5d-935a-4428-91f7-ad590d461d20",
   "metadata": {},
   "source": [
    "1. Improve performance of the ML Model.\n",
    "2. Understand advanced optimization techniques.\n",
    "3. Learn how to use various open source packages.\n",
    "4. Participate and Lead in data science competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82901c5d-32d0-4b88-986d-da513ef1d6fb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packages Used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31cfa2e6-024a-4dc5-aa25-a5a9a83ce023",
   "metadata": {},
   "source": [
    "1. [scikit-learn](https://scikit-learn.org/stable/)\n",
    "2. [scikit-optimize](https://scikit-optimize.github.io/stable/)\n",
    "3. [Hyperopt](http://hyperopt.github.io/hyperopt/)\n",
    "4. [OPTUNA](https://optuna.org)\n",
    "5. [KerasTuner](https://keras.io/keras_tuner/)\n",
    "6. [SMAC3](https://automl.github.io/SMAC3/main/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0322ee-98cc-4a0b-848f-4ab1d5c7b565",
   "metadata": {},
   "source": [
    "## More Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a880cd48-a033-490e-8574-6aca025e9203",
   "metadata": {},
   "source": [
    "**1. How can I learn more about relevant Machine Learning and Data Science skills?**\n",
    "* [Resources to learn more about Python programming](https://trainindata.medium.com/discover-the-best-resources-to-learn-python-for-data-science-35b87d38fadf)\n",
    "* [Resources to learn more about Machine Learning](https://trainindata.medium.com/find-out-the-best-resources-to-learn-machine-learning-cd560beec2b7)\n",
    "* [Resources to learn more about Data Science](https://trainindata.medium.com/discover-the-best-resources-to-learn-data-science-2978499fc804)\n",
    "\n",
    "\n",
    "Keep in mind that machine learning is a very extensive field, and therefore you will likely need to visit multiple courses and resources to get a broad understanding of the different algorithms.\n",
    "\n",
    "\n",
    "**2) I would like to know more about variable pre-processing and data cleaning for machine learning. What can I do?**\n",
    "\n",
    "\n",
    "In the final section of this course you will find a link to comprehensive course on feature engineering. Meanwhile, have a look at these articles:\n",
    "\n",
    "\n",
    "* [Feature Engineering for Machine Learning: A Comprehensive Overview](https://trainindata.medium.com/feature-engineering-for-machine-learning-a-comprehensive-overview-a7ad04c896f8)\n",
    "* [Best Resources to Learn about Feature Engineering for Machine Learning](https://trainindata.medium.com/best-resources-to-learn-feature-engineering-for-machine-learning-6b4af690bae7)\n",
    "* [Practical Code Implementation of Feature Engineering Techniques with Python](https://towardsdatascience.com/practical-code-implementations-of-feature-engineering-for-machine-learning-with-python-f13b953d4bcd)\n",
    "\n",
    "\n",
    "**3) I would like to learn more about feature selection for machine learning. Do you know of good resources?**\n",
    "\n",
    "\n",
    "In the final section of this course you will find a link to comprehensive course on feature selection. Meanwhile, have a look at these articles:\n",
    "* [Feature Selection for Machine Learning: A Comprehensive Overview](https://trainindata.medium.com/feature-selection-for-machine-learning-a-comprehensive-overview-bd571db5dd2d)\n",
    "\n",
    "\n",
    "[You can also find resources to improve your skills further.](https://trainindata.medium.com/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bfa1be-da0e-4161-a0a7-8d01d7d1ad3c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 1. Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d102df7f",
   "metadata": {},
   "source": [
    "[scikit-learn metrics](https://scikit-learn.org/stable/modules/model_evaluation.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb815f7e-e8d9-4452-afd0-a0ac4097b941",
   "metadata": {},
   "source": [
    "## a. Classification Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ec73aa",
   "metadata": {},
   "source": [
    "Two types of Classification Metrics, that are:\n",
    "\n",
    "1. dependent of the probability threshold\n",
    "    * Accuracy\n",
    "    * Precision, Recall, f-score\n",
    "    * False Positive Rate (FPR) and False Negative Rate (FNR)\n",
    "2. independent of the probability threshold\n",
    "    * ROC-AUC\n",
    "    \n",
    "**Accuracy:** is the percentage of correct predictions.</br>\n",
    "90% accuracy means out of 10 predictions made by the Model, 9 of them were correct. 90/100, 900/1000 ...\n",
    "> accuracy = no. of correct predictions / total no. of predictions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b30c45",
   "metadata": {},
   "source": [
    "<img src=\"images/confusion_matrix.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d04e35e1",
   "metadata": {},
   "source": [
    "**Precision or Positive Predicted Value = TP / (TP + FP)** \n",
    "> * is True Positive divided by Total no. of Positive Classes predicted by the Model (no matter it was actually positive or not) i.e True Positive + False Positive.\n",
    "\n",
    "**Recall or Sensitivity or True Positive Rate = TP / (TP+FN)** \n",
    "> * is True Positive divided by Total no. of actual Positive classes i,e True Positive + False Negative (Actually it was Positive but predicted Negative).\n",
    "> * E.g: Suppose your gf asks you to recall how many number of times you both went for a date. So, the total number of dates you were able to recall correctly will be your True Positive(TP) and Total no. of dates you recalled incorrectly will be your False Negative(FN) i.e they were positive but you recalled it incorrectly (negative).\n",
    "\n",
    "**f-score = 2 x precision x recall / precision + recall** \n",
    "> is weighted harmonic mean of precision and recall.\n",
    "\n",
    "**False Positive Rate (FPR) = FP / (FP + TN)**\n",
    "> is out of all the Negative Class (TN + FP) , how many of them were incorrectly predicted as Positive though they were Negative i,e FP\n",
    "\n",
    "**False Negative Rate (FNR) = FN / (FN + TP)**\n",
    ">is out of all the Positive Class (TP + FN) , how many of them were incorrectly predicted as Negative though they were Positive i,e FP\n",
    "\n",
    "**ROC-AUC:**\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0140461",
   "metadata": {},
   "source": [
    "<img src=\"images/roc_auc_curve.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a932a5",
   "metadata": {},
   "source": [
    "**Log Loss Function $$J(\\theta) = -\\frac{1}{m} \\sum_{i=1}^m y^{(i)}\\log (h(z(\\theta)^{(i)})) + (1-y^{(i)})\\log (1-h(z(\\theta)^{(i)}))\\tag{5} $$**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2ec60b",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "> * Accuracy, Precision, Recall, f1-score, ROC-AUC curve **inversely proportional to** model performance i.e (smaller the value greater the model)\n",
    "> * FPR, FNR, Log Loss **directly proportional to** model performance i.e (greater the value greater the model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc920be-669f-4009-94cf-48b7ea37f6b5",
   "metadata": {},
   "source": [
    "## b. Regression Performance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37da7324",
   "metadata": {},
   "source": [
    "<img src=\"images/regression.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f334512",
   "metadata": {},
   "source": [
    "**Note:**\n",
    "* **MSE, RMSE, MAE is inversely proportional to model performance** We want to minimize the value of MSE, RMSE, MAE because closer the value to 0 greater is the performance of the model, since the loss is minimum near zero.\n",
    "> **MSE, RMSE, MAE:** are the measure of the distance between the true label and predicted label, lower the distance (value) better the model.\n",
    "* **R2 score is directly proportional to model performance** We want to increase the value of r2 since closer the value to 1 greater is the model.\n",
    "> **R2 score:** is the measure of variability of the dataset. If r2 score of the model is 0.4 it means that the model explains 40% of the variability present in the dataset and the rest it cannot. And if r2 score is 1 that means the model is a perfect model because it explains 100% of the variability present in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130d07ae-b5b4-4339-91d7-45a37807a219",
   "metadata": {},
   "source": [
    "## c. Create your own Perfrormance Metrics with scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eacdbf30-8f11-45c1-af62-f17130f0d17d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 2. Cross Validation Schemes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e4c586-cb17-4a35-b4ca-dfe062d52b8e",
   "metadata": {
    "tags": []
   },
   "source": [
    "## a. K-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2c6f721-14f8-4542-86b8-9da48e0fb877",
   "metadata": {},
   "source": [
    "## b. Leave One Out (LOOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eb9d877-25ac-46d1-bb90-71ad9e282ee0",
   "metadata": {},
   "source": [
    "## c. Leave P Out (LPOCV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d13c72-ae70-4459-9b8e-33e0577b8c68",
   "metadata": {},
   "source": [
    "## d. Repeated K-Fold CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe91c16f-08c7-439f-9809-b6aea05104bd",
   "metadata": {},
   "source": [
    "## e. Stratified Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4350cfd8-6568-474b-8102-b25a6651e14f",
   "metadata": {},
   "source": [
    "## f. Nested Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19ff56be-be96-41bd-83c0-34ffb2535ff1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 3. Basic Search Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a69a7896-dad9-4daf-a1b8-f4b909294500",
   "metadata": {},
   "source": [
    "## a. Manual Search CV "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760c9a7a-b923-4e57-a54a-6fe7d3bc7436",
   "metadata": {},
   "source": [
    "## b. Grid Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30ddd8d-df1e-4e7d-8805-e288fc8a5d33",
   "metadata": {},
   "source": [
    "## c. Random Search CV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c82a749-7203-42d3-9a9d-43c18efc2785",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Bayesian Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98ed847-852f-4133-bc7f-45c13a29dc59",
   "metadata": {},
   "source": [
    "# 5. Advanced Optimization Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7685c8f-bd1e-4787-af9f-31af45d5060b",
   "metadata": {},
   "source": [
    "# 6. Open Source Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d285673d-f0b0-4591-8c44-2fc2036467f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
